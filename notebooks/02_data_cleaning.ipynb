{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9eea90-81ad-4704-9c44-960f6464d217",
   "metadata": {},
   "source": [
    "***************************************************************************************\n",
    "Jupyter Notebooks from the Metadata for Everyone project\n",
    "\n",
    "Code:\n",
    "* Dennis Donathan II (https://orcid.org/0000-0001-8042-0539)\n",
    "\n",
    "Project team: \n",
    "* Juan Pablo Alperin (https://orcid.org/0000-0002-9344-7439)\n",
    "* Dennis Donathan II (https://orcid.org/0000-0001-8042-0539)\n",
    "* Mike Nason (https://orcid.org/0000-0001-5527-8489)\n",
    "* Julie Shi (https://orcid.org/0000-0003-1242-1112)\n",
    "* Marco Tullney (https://orcid.org/0000-0002-5111-2788)\n",
    "\n",
    "Last updated: xxx\n",
    "***************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01402dd8",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "The raw csv consists of 3 columns:\n",
    "\n",
    "* Index\n",
    "\n",
    "* DOI\n",
    "\n",
    "* XML\n",
    "\n",
    "The XML record has a lot of information, but not all is relevant for this study. We will extract from each record the metadata that is relevant and then format it in a nested dictionary. The resulting dictionary's schema looks like this:\n",
    "\n",
    "```\n",
    "{\n",
    "    'doi': str | None,\n",
    "    'authors': list[dict[given_name, surname, sequence, affiliation]] | None,\n",
    "    'abstracts': list[str] | None,\n",
    "    'journal_lang': str | None,\n",
    "    'article_lang': str | None,\n",
    "    'abstract_langs': list[str] | None,\n",
    "    'publisher_name': str | None,\n",
    "    'journal_title': str | None,\n",
    "    'article_title': str | None\n",
    "    }\n",
    "```\n",
    "Due to the size of the dataset, we'll use [Dask](https://docs.dask.org/en/stable/index.html) to load in the csv and preform the metadata extraction functions defined below. Depending on the hardware resources, the time to load in the data and preform the extraction can vary. The parameter `blocksize=250MB` found within the `df` variable in the `clean_csv` function can be altered accordingly. 250MB is a somewhat neutral value in that most computing systems can run the code comfortably, but it will take multiple hours to run.\n",
    "\n",
    "Now we will load in our packages and set up our paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dfabfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Set up directories\n",
    "data_dir = Path('../data')\n",
    "input_dir = data_dir / 'input'\n",
    "output_dir = data_dir / 'output'\n",
    "\n",
    "csv_path = input_dir / 'allv3.csv'\n",
    "parquet_path = input_dir / '02_cleaned_data.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb951f1",
   "metadata": {},
   "source": [
    "## Extraction Functions\n",
    "\n",
    "Each function is named according to which piece of metadata it will extract. Then they are all called within the `__metadata` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560558c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __authors(soup: 'bs4.BeautifulSoup') -> list[dict] | None:\n",
    "    \"\"\"Helper function to extract relevant author metadata from\n",
    "    XML records.\n",
    "\n",
    "    Args:\n",
    "        record (str): An individual metadata record in XML format.\n",
    "\n",
    "    Returns:\n",
    "        list[dict] | None : A list of nested dictionaries containing the relevant author metadata.\n",
    "                            If no authors are present, None is returned.\n",
    "    \"\"\"\n",
    "    author_list = []\n",
    "    #soup = BeautifulSoup(record, 'xml')\n",
    "    authors = soup.find('contributors')\n",
    "    if authors:\n",
    "        first_authors = authors.find_all('person_name', sequence='first')\n",
    "        additional_authors = authors.find_all('person_name', sequence='additional')\n",
    "        for i in first_authors:\n",
    "            name_dict = {\n",
    "                'given_name': None,\n",
    "                'surname': None,\n",
    "                'sequence': None,\n",
    "                'affiliation': None\n",
    "            }\n",
    "            for k in name_dict:\n",
    "                if k =='sequence':\n",
    "                    name_dict[k] = 'first'\n",
    "                else:\n",
    "                    if i.find(k):\n",
    "                        name_dict[k] = i.find(k).get_text()\n",
    "                    else:\n",
    "                        continue\n",
    "            author_list.append(name_dict)\n",
    "        for i in additional_authors:\n",
    "            name_dict = {\n",
    "                'given_name': None,\n",
    "                'surname': None,\n",
    "                'sequence': None,\n",
    "                'affiliation': None\n",
    "            }\n",
    "            for k in name_dict:\n",
    "                if k =='sequence':\n",
    "                    name_dict[k] = 'additional'\n",
    "                else:\n",
    "                    if i.find(k):\n",
    "                        name_dict[k] = i.find(k).get_text()\n",
    "                    else:\n",
    "                        continue\n",
    "            author_list.append(name_dict)\n",
    "    if len(author_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return author_list\n",
    "    \n",
    "def __abstracts(soup: 'bs4.BeautifulSoup') -> list[str] | None:\n",
    "    \"\"\"Helper function that extracts all abstracts from XML records.\n",
    "\n",
    "    Args:\n",
    "        record (str): An individual metadata record in XML format.\n",
    "\n",
    "    Returns:\n",
    "        list[str] | None: Returns a list of all abstracts within a record.\n",
    "                        If there is no abstract within a record,\n",
    "                        then None is returned.\n",
    "    \"\"\"\n",
    "    #soup = BeautifulSoup(record, 'xml')\n",
    "    abstracts = soup.find_all('jats:abstract')\n",
    "    text = []\n",
    "    if abstracts:\n",
    "        for i in abstracts:\n",
    "            text.append(i.get_text())\n",
    "    else:\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "def __languages(soup: 'bs4.BeautifulSoup') -> dict:\n",
    "    \"\"\"Helper function that extracts Language codes from multiple fields \n",
    "    within an XML record\n",
    "\n",
    "    Args:\n",
    "        record (str): An individual metadata record in XML format.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the language codes for three \n",
    "            different metadata fields.\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    try: \n",
    "        #soup = BeautifulSoup(record, 'xml')\n",
    "        journal = soup.find('journal_metadata')\n",
    "        if journal:\n",
    "            ret['journal_lang'] = journal.get('language')\n",
    "        else:\n",
    "            ret['journal_lang'] = None\n",
    "            \n",
    "        article = soup.find('journal_article')\n",
    "        if article: \n",
    "            ret['article_lang'] = article.get('language')\n",
    "        else:\n",
    "            ret['article_lang'] = None\n",
    "\n",
    "        abstracts = soup.find_all('jats:abstract')\n",
    "        if abstracts: \n",
    "            langs = []\n",
    "            for abstract in abstracts: \n",
    "                langs.append(abstract.get('xml:lang'))\n",
    "                langs = [l for l in langs if l is not None]\n",
    "                \n",
    "            if len(langs) == 0:\n",
    "                langs = None\n",
    "            ret['abstract_langs'] = langs\n",
    "        else:\n",
    "            ret['abstract_langs'] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        ret['err'] = type(e).__name__\n",
    "\n",
    "    return ret\n",
    "\n",
    "def __titles(soup: 'bs4.BeautifulSoup') -> dict | None:\n",
    "    \"\"\"Helper function to extract various titles from XML records.\n",
    "\n",
    "    Args:\n",
    "        record (str): An individual metadata record in XML format.\n",
    "\n",
    "    Returns:\n",
    "        dict | None: A dictionary containing titles and labels, or None\n",
    "                    if no titles are present.\n",
    "    \"\"\" \n",
    "    #soup = BeautifulSoup(record, 'xml')\n",
    "    titles = {}\n",
    "    try:\n",
    "        publisher = soup.find('crm-item', attrs={'name': 'publisher-name'})\n",
    "        if publisher:\n",
    "            titles['publisher_name'] = publisher.get_text()\n",
    "        else:\n",
    "            titles['publisher_name'] = None\n",
    "        journal = soup.find('journal_metadata')\n",
    "        if journal:\n",
    "            titles['journal_title'] = journal.find('full_title').get_text()\n",
    "        else:\n",
    "            titles['journal_title'] = None\n",
    "        article = soup.find('titles')\n",
    "        if article:\n",
    "            article = article.find_all('title')\n",
    "            titles['article_title'] = [i.get_text() for i in article]\n",
    "        else:\n",
    "            titles['article_title'] = None\n",
    "        return titles\n",
    "    except Exception as err:\n",
    "        return err\n",
    "\n",
    "def __metadata(record: str) -> dict:\n",
    "    try:\n",
    "        soup = BeautifulSoup(record, 'xml')\n",
    "        doi = soup.find('doi')\n",
    "        authors = __authors(soup)\n",
    "        abstracts = __abstracts(soup)\n",
    "        languages = __languages(soup)\n",
    "        titles = __titles(soup)\n",
    "        final_record = {'doi': doi.get_text() if doi else None,\n",
    "                        'authors': authors,\n",
    "                        'abstracts': abstracts,\n",
    "                        'journal_lang': languages['journal_lang'],\n",
    "                        'article_lang': languages['article_lang'],\n",
    "                        'abstract_langs': languages['abstract_langs'],\n",
    "                        'publisher_name': titles['publisher_name'],\n",
    "                        'journal_title': titles['journal_title'],\n",
    "                        'article_title': titles['article_title']}\n",
    "        return final_record\n",
    "    except TypeError as err:\n",
    "        if type(record) == 'NAType':\n",
    "            return None\n",
    "        else:\n",
    "            print(err)\n",
    "\n",
    "\n",
    "def clean_csv(csv_path: str, metadata_parquet_path: str):\n",
    "    # Here is the df variable containing the blocksize parameter.\n",
    "    df = dd.read_csv(csv_path, names=['index', 'DOI', 'xml'], blocksize='250MB')\n",
    "    metadata = df['xml'].map(__metadata, meta=('metadata', 'object')).compute()\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    metadata_df.to_parquet(metadata_parquet_path, index=False)\n",
    "    return metadata_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49fed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell runs the functions and saves the new data to a parquet.\n",
    "Depending on the value set in the blocksize parameter,\n",
    "this may take some time.\n",
    "\"\"\"\n",
    "\n",
    "df = clean_csv(csv_path, parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "447af80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(parquet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6cc2f",
   "metadata": {},
   "source": [
    "## Conferences\n",
    "There are a couple 'Conferences' and 'Proceedings' in the *journal_title* column. Let's take a look at just how many records remain in our dataset are from these journals/containers.\n",
    "\n",
    "Additionally, we see a few records from the journal *ChemInform*, a journal that publishes chemistry abstracts, we'll check to see if any of those records remain as well.\n",
    "\n",
    "We'll use a keyword search in the *journal_title* column to find these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conferences = df.loc[(df.journal_title.str.contains(r'conference|ChemInform|news|CrossRef Listing of Deleted DOIs', \n",
    "                                                    regex=True, case=False)) |\n",
    "                                                    (df.publisher_name == 'EDP Sciences')]\n",
    "conferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d86b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(conferences.index, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb49e1",
   "metadata": {},
   "source": [
    "Looks great! Now we'll save our cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "827651bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(parquet_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
